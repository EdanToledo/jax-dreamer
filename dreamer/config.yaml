defaults:
  # Train Script
  log_dir: results
  seed: 0
  task: pendulum.swingup
  time_limit: 1000
  action_repeat: 2
  steps: 1e6
  training_steps_per_epoch: 2.5e4
  evaluation_steps_per_epoch: 1e4
  prefill: 5000
  train_every: 1000
  update_steps: 100
  replay: {capacity: 1000, batch: 32, sequence_length: 50}
  jit: True
  render_episodes: 0
  evaluate_model: True
  precision: 16
  initialization: glorot

  # World Model
  rssm: {hidden: 200, deterministic_size: 200, stochastic_size: 30, min_stddev: 0.05}
  rssm_posterior: {stddev: 1e-5, learnable: True}
  rssm_prior: {stddev: 0.1, learnable: True}
  params_kl_scale: 1e-8
  params_free_kl: 1.0
  rssm_posterior_samples: 10
  model_opt: {lr: 6e-4, eps: 1e-7, clip: 100}
  encoder: {depth: 32, kernels: [4, 4, 4, 4]}
  decoder: {depth: 32, kernels: [5, 5, 6, 6]}
  reward: {output_sizes: [400, 400]}
  terminal: {output_sizes: [400, 400, 400]}
  free_kl: 1.0
  kl_scale: 1.0

  # Actor-Critic
  actor: {output_sizes: [400, 400, 400, 400], min_stddev: 1e-4}
  critic: {output_sizes: [400, 400, 400]}
  actor_opt: {lr: 8e-5, eps: 1e-7, clip: 100}
  critic_opt: {lr: 8e-5, eps: 1e-7, clip: 100}
  discount: 0.99
  lambda_: 0.95
  imag_horizon: 15

  # Optimism
  optimistic_model_opt: {lr: 6e-4, eps: 1e-7, clip: 100}
  constraint_lr: 3e-4
  mahalanobis_threshold: 0.1
  initial_lagrangian: 1e-8

debug:
  jit: False
  time_limit: 50
  training_steps_per_epoch: 100
  evaluation_steps_per_epoch: 0
  prefill: 100
  train_every: 100
  update_steps: 2
  render_episodes: 0
  replay: {capacity: 10, batch: 10, sequence_length: 8}